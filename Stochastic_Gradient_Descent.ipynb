{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4H8LXxHyNzIwrJumrXOBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raffa-R/Optimization-Algorithm-for-ML/blob/main/Stochastic_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "\n",
        "#np.random.seed(202404)\n",
        "np.random.seed(42024)\n",
        "\n",
        "trueW = np.array([1,2,3,4,5])\n",
        "def generate():\n",
        "  x = np.random.randn(len(trueW))\n",
        "  y = trueW.dot(x) + np.random.randn()\n",
        "  #print('example',x,y)\n",
        "  return(x,y)\n",
        "\n",
        "trainExamples = [generate() for i in range(1000000)]"
      ],
      "metadata": {
        "id": "l_CNVBl8CpLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE2PpLs0WaXK",
        "outputId": "5b6110fd-4f1e-4755-b6d1-65d7f4976b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: w [0.98760383 1.97714736 2.94424079 3.99250552 5.01803744], F[0.98760383 1.97714736 2.94424079 3.99250552 5.01803744] = 0.12478346983477888, gradientF = [ 3.75191892  3.69069479 -1.27108508 -0.30437416  0.56153566]\n",
            "epoch 1: w [0.99127169 2.04013748 2.96731257 3.99865317 4.99176852], F[0.99127169 2.04013748 2.96731257 3.99865317 4.99176852] = 2.2670386799077566, gradientF = [ 0.16718195  0.20889932 -0.00751443 -0.08568657  0.14701766]\n",
            "epoch 2: w [0.98917292 2.021703   3.01354534 4.03409884 4.99382765], F[0.98917292 2.021703   3.01354534 4.03409884 4.99382765] = 0.005732569927403092, gradientF = [-0.38375498 -0.62538202  2.24573587 -1.42136154  0.89648991]\n",
            "epoch 3: w [1.01781231 1.96281489 2.99896581 4.01971938 5.00662772], F[1.01781231 1.96281489 2.99896581 4.01971938 5.00662772] = 0.0024964669199490537, gradientF = [-0.55503488  0.12098068  0.27581215  0.17000262 -0.44649561]\n",
            "epoch 4: w [0.9791741  2.0058997  2.99102692 3.99772245 5.00624213], F[0.9791741  2.0058997  2.99102692 3.99772245 5.00624213] = 0.014496099223895872, gradientF = [-5.49621976 -1.01766894 -0.44405386 -1.35180978 -4.58042506]\n",
            "epoch 5: w [0.97642428 2.04237127 2.98509136 4.00832128 5.01876464], F[0.97642428 2.04237127 2.98509136 4.00832128 5.01876464] = 7.655108850650304e-06, gradientF = [-0.57781183 -0.33377239 -0.12809623 -0.37475721  0.43848045]\n",
            "epoch 6: w [1.01100062 2.01805873 3.01892851 4.01346174 4.98878372], F[1.01100062 2.01805873 3.01892851 4.01346174 4.98878372] = 0.15706665499609385, gradientF = [ 0.1183107  -0.14249744  0.83900242 -0.09627619 -0.79601608]\n",
            "epoch 7: w [0.99184969 2.00233831 3.00727769 3.99254039 4.98168199], F[0.99184969 2.00233831 3.00727769 3.99254039 4.98168199] = 0.1887529983010579, gradientF = [-0.09245345  0.15207769  0.24093585 -0.41949141  0.5084499 ]\n",
            "epoch 8: w [0.97977897 1.99934575 2.98622199 3.98946565 5.04310687], F[0.97977897 1.99934575 2.98622199 3.98946565 5.04310687] = 0.1592513814557849, gradientF = [ 0.32129213  0.6693975  -0.10454189 -0.10795674 -0.34940232]\n",
            "epoch 9: w [1.01322991 2.00111274 2.99193253 3.99135985 4.99363779], F[1.01322991 2.00111274 2.99193253 3.99135985 4.99363779] = 8.634515621596397e-05, gradientF = [-0.54346318  0.7915686  -1.2205421  -1.51550015 -2.54318709]\n",
            "epoch 10: w [1.00016067 2.03499418 2.99163284 4.03013962 4.99664385], F[1.00016067 2.03499418 2.99163284 4.03013962 4.99664385] = 0.14769781936719534, gradientF = [ 1.16760477 -1.2315166   1.72939549 -0.16689508  3.84966282]\n",
            "epoch 11: w [1.00887991 2.02042594 2.96248733 4.03953595 4.98589089], F[1.00887991 2.02042594 2.96248733 4.03953595 4.98589089] = 2.794191933498455, gradientF = [ 2.26794097  0.09696754 -0.60173203 -1.71163322 -0.36128948]\n",
            "epoch 12: w [0.99612885 1.99572395 2.98265888 4.00759369 4.98011426], F[0.99612885 1.99572395 2.98265888 4.00759369 4.98011426] = 0.10253336139382944, gradientF = [ 2.02872129 -0.46492902 -1.32789297  0.62593262 -1.29564576]\n",
            "epoch 13: w [1.01062855 2.00209724 2.99238644 3.99548839 5.00864542], F[1.01062855 2.00209724 2.99238644 3.99548839 5.00864542] = 1.5936674814065688, gradientF = [-0.20125839  0.18813667 -0.75256555  0.51839606  0.11872219]\n",
            "epoch 14: w [0.96830534 2.0004001  2.99196936 4.00876807 4.98715331], F[0.96830534 2.0004001  2.99196936 4.00876807 4.98715331] = 0.8273235867700082, gradientF = [ 1.01157455  1.5436725  -1.15824422 -1.19951047 -0.84376444]\n",
            "epoch 15: w [1.01095528 1.99105947 3.02400095 3.98600107 5.00027124], F[1.01095528 1.99105947 3.02400095 3.98600107 5.00027124] = 0.023135272692492527, gradientF = [ 0.7317655   0.62167941 -0.01371118  0.16265382 -2.48532471]\n",
            "epoch 16: w [0.97704765 1.97741978 3.00787821 4.00963534 4.97989512], F[0.97704765 1.97741978 3.00787821 4.00963534 4.97989512] = 0.4794683323207856, gradientF = [-2.08322256  2.10426632  1.42095986 -1.45174318 -2.81916428]\n",
            "epoch 17: w [1.00183281 2.00082531 3.01311795 4.00322215 4.99443207], F[1.00183281 2.00082531 3.01311795 4.00322215 4.99443207] = 0.07035685164683891, gradientF = [ 0.41234308  0.10716365 -0.42419232  0.46136232  0.12393049]\n",
            "epoch 18: w [1.00451327 1.99165306 3.01130989 3.9766038  5.00749626], F[1.00451327 1.99165306 3.01130989 3.9766038  5.00749626] = 1.4048278762689297, gradientF = [ 3.94529239  2.15874867  1.99628657  1.08155583 -1.35404291]\n",
            "epoch 19: w [0.99713305 1.99072458 3.00541473 3.98109325 5.00804274], F[0.99713305 1.99072458 3.00541473 3.98109325 5.00804274] = 0.911243686074852, gradientF = [-0.90756501  1.20994178 -1.15373697 -1.77641344  1.06094138]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#trainExamples = [\n",
        "#    (1,1),\n",
        "#    (2,3),\n",
        "#    (4,3)\n",
        "#]\n",
        "\n",
        "def phi(x):\n",
        "  return np.array(x)\n",
        "\n",
        "def initialWeightVector():\n",
        "  return np.ones(len(trueW))\n",
        "\n",
        "#def trainLoss(w):\n",
        "  #return 1.0 / len(trainExamples) * sum((w.dot(phi(x)) - y)**2 for x,y in trainExamples)\n",
        "\n",
        "#def gradientTrainLoss(w):\n",
        "  #return 1.0 / len(trainExamples) * sum(2 * (w.dot(phi(x)) - y) * phi(x) for x,y in trainExamples)\n",
        "\n",
        "def loss(w):\n",
        "  x,y = random.choice(trainExamples)\n",
        "  return (w.dot(phi(x)) - y)**2\n",
        "\n",
        "def gradientLoss(w):\n",
        "  x,y = random.choice(trainExamples)\n",
        "  return 2 * (w.dot(phi(x)) - y)* phi(x)\n",
        "\n",
        "def gradientDescent(F, gradientF, initialWeightVector):\n",
        "  w = initialWeightVector()\n",
        "  eta = 0.1\n",
        "  for t in range(200):\n",
        "    value = F(w)\n",
        "    gradient = gradientF(w)\n",
        "    w = w - eta * gradient\n",
        "    print(f'epoch {t}: w {w}, F{w} = {value}, gradientF = {gradient}')\n",
        "\n",
        "def stochasticGradientDescent(f, gradientf, n, initialWeightVector):\n",
        "  w = initialWeightVector()\n",
        "  numUpdates = 0\n",
        "  for t in range(20):\n",
        "    for i in range(n):\n",
        "      value = f(w)\n",
        "      gradient = gradientf(w)\n",
        "      numUpdates += 1\n",
        "      eta = 1 / math.sqrt(numUpdates)\n",
        "      w = w - eta * gradient\n",
        "    print(f'epoch {t}: w {w}, F{w} = {value}, gradientF = {gradient}')\n",
        "\n",
        "#gradientDescent(trainLoss, gradientTrainLoss, initialWeightVector)\n",
        "stochasticGradientDescent(loss, gradientLoss, len(trainExamples), initialWeightVector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "# Sample array\n",
        "my_array = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Selecting a random element from the array\n",
        "random_element = random.choice(my_array)\n",
        "\n",
        "print(\"Random element:\", random_element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2t-C1-kILVl",
        "outputId": "cd2c86bd-db23-4fd3-d905-9e7a759c4e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random element: 3\n"
          ]
        }
      ]
    }
  ]
}